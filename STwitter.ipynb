{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f797108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk \n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings; warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3f2ed826",
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.read_csv('train.csv', encoding =('ISO-8859-1'),low_memory =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ae4ad8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding =('ISO-8859-1'),low_memory =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3de853d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>is so sad for my APL friend.............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I missed the New Moon trailer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>i think mi bf is cheating on me!!!       T_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299984</th>\n",
       "      <td>299996</td>\n",
       "      <td>@martine2323 'Morning, Petal - work brought me here (banking IT). First Holland, then here.  And I just stayed... and stayed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299985</th>\n",
       "      <td>299997</td>\n",
       "      <td>@martinetrene Hello,little bird!  i finally started on Rick's movie this morning &amp;amp; got to the part where you are about to save the day! &amp;lt;3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299986</th>\n",
       "      <td>299998</td>\n",
       "      <td>@MartinezMayra I got u at work when I go buy one u know ima get u one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299987</th>\n",
       "      <td>299999</td>\n",
       "      <td>@martinfaux He certainly gives good interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299988</th>\n",
       "      <td>300000</td>\n",
       "      <td>@martinfaux we've the feedback about our browser compatibility story in v3 loud and clear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>299989 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ItemID  \\\n",
       "0            1   \n",
       "1            2   \n",
       "2            3   \n",
       "3            4   \n",
       "4            5   \n",
       "...        ...   \n",
       "299984  299996   \n",
       "299985  299997   \n",
       "299986  299998   \n",
       "299987  299999   \n",
       "299988  300000   \n",
       "\n",
       "                                                                                                                                            SentimentText  \n",
       "0                                                                                                                is so sad for my APL friend.............  \n",
       "1                                                                                                                        I missed the New Moon trailer...  \n",
       "2                                                                                                                                 omg its already 7:30 :O  \n",
       "3                              .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...  \n",
       "4                                                                                                            i think mi bf is cheating on me!!!       T_T  \n",
       "...                                                                                                                                                   ...  \n",
       "299984                   @martine2323 'Morning, Petal - work brought me here (banking IT). First Holland, then here.  And I just stayed... and stayed...   \n",
       "299985  @martinetrene Hello,little bird!  i finally started on Rick's movie this morning &amp; got to the part where you are about to save the day! &lt;3  \n",
       "299986                                                                             @MartinezMayra I got u at work when I go buy one u know ima get u one   \n",
       "299987                                                                                                     @martinfaux He certainly gives good interview   \n",
       "299988                                                         @martinfaux we've the feedback about our browser compatibility story in v3 loud and clear   \n",
       "\n",
       "[299989 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4beaf8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL friend.............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trailer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!       T_T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99984</th>\n",
       "      <td>99996</td>\n",
       "      <td>0</td>\n",
       "      <td>@Cupcake  seems like a repeating problem   hope you're able to find something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>99997</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll see you then, Duno where the hell Kateyy is!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99986</th>\n",
       "      <td>99998</td>\n",
       "      <td>0</td>\n",
       "      <td>@CuPcAkE_2120 ya i thought so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99987</th>\n",
       "      <td>99999</td>\n",
       "      <td>1</td>\n",
       "      <td>@Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99988</th>\n",
       "      <td>100000</td>\n",
       "      <td>1</td>\n",
       "      <td>@cupcake_kayla haha yes you do</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99989 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ItemID  Sentiment  \\\n",
       "0           1          0   \n",
       "1           2          0   \n",
       "2           3          1   \n",
       "3           4          0   \n",
       "4           5          0   \n",
       "...       ...        ...   \n",
       "99984   99996          0   \n",
       "99985   99997          1   \n",
       "99986   99998          0   \n",
       "99987   99999          1   \n",
       "99988  100000          1   \n",
       "\n",
       "                                                                                                                                    SentimentText  \n",
       "0                                                                                                        is so sad for my APL friend.............  \n",
       "1                                                                                                                I missed the New Moon trailer...  \n",
       "2                                                                                                                         omg its already 7:30 :O  \n",
       "3                      .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...  \n",
       "4                                                                                                    i think mi bf is cheating on me!!!       T_T  \n",
       "...                                                                                                                                           ...  \n",
       "99984                                                              @Cupcake  seems like a repeating problem   hope you're able to find something.  \n",
       "99985  @cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll see you then, Duno where the hell Kateyy is!  \n",
       "99986                                                                                                              @CuPcAkE_2120 ya i thought so   \n",
       "99987                                                                               @Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me.   \n",
       "99988                                                                                                             @cupcake_kayla haha yes you do   \n",
       "\n",
       "[99989 rows x 3 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a3ba344b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ItemID', 'Sentiment', 'SentimentText'], dtype='object')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns in data\n",
    "\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "21b6df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length oif data is 99989\n"
     ]
    }
   ],
   "source": [
    "# length of dataset\n",
    "\n",
    "print(\"Length oif data is\",len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "36e1e40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99989, 3)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of data\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea34ec2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of        ItemID  Sentiment  \\\n",
       "0           1          0   \n",
       "1           2          0   \n",
       "2           3          1   \n",
       "3           4          0   \n",
       "4           5          0   \n",
       "...       ...        ...   \n",
       "99984   99996          0   \n",
       "99985   99997          1   \n",
       "99986   99998          0   \n",
       "99987   99999          1   \n",
       "99988  100000          1   \n",
       "\n",
       "                                                                                                                                    SentimentText  \n",
       "0                                                                                                        is so sad for my APL friend.............  \n",
       "1                                                                                                                I missed the New Moon trailer...  \n",
       "2                                                                                                                         omg its already 7:30 :O  \n",
       "3                      .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...  \n",
       "4                                                                                                    i think mi bf is cheating on me!!!       T_T  \n",
       "...                                                                                                                                           ...  \n",
       "99984                                                              @Cupcake  seems like a repeating problem   hope you're able to find something.  \n",
       "99985  @cupcake__ arrrr we both replied to each other over different tweets at the same time  , i'll see you then, Duno where the hell Kateyy is!  \n",
       "99986                                                                                                              @CuPcAkE_2120 ya i thought so   \n",
       "99987                                                                               @Cupcake_Dollie Yes. Yes. I'm glad you had more fun with me.   \n",
       "99988                                                                                                             @cupcake_kayla haha yes you do   \n",
       "\n",
       "[99989 rows x 3 columns]>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data information\n",
    "\n",
    "train.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1a5a9892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ItemID            int64\n",
       "Sentiment         int64\n",
       "SentimentText    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datatypes of all columns\n",
    "\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3f3289b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for Null values\n",
    "\n",
    "np.sum(train.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7bbddacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of columns in the data is:   3\n",
      "Count of rows in the data is:   99989\n"
     ]
    }
   ],
   "source": [
    "# Rows and columns in the dataset\n",
    "\n",
    "print('Count of columns in the data is:  ', len(train.columns))\n",
    "print('Count of rows in the data is:  ', len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dcb82",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "156f5bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='Sentiment', data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a42d3a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4768                                                                                                 Talk about being awakened onn the wrong side of the bed.\n",
       "99631                        @Crownlessking badtimes mate  its not fair and nothing anyone can say is guna make it easier or better but we're here for you xx\n",
       "29400                                                                                                                         @AClockworkToad How are you?   \n",
       "29483                                                      @acryfromthesoul I know.  As long as my memory is still intact, I'll be able to relive it though. \n",
       "25399                                                                               @30STMWithJared ah that sucks  just going out now, I'll be on in a bit  x\n",
       "63456                                                    @BMatt95 A users who doesnt know how to use the report button should not be on the forum. now GTFO! \n",
       "79016                                                                                                    @CelestialBeard Money can be used for great things. \n",
       "50879                                              @asteris I know but whoever wrote the title prob. thought it sounded more impressive and they were right. \n",
       "29313                                                                                                                    @acreature it's all part of the fun \n",
       "81256                                                    @asher_book hey asher, when will vfactory's album out? i can't wait. u guys songs are awesomeness!! \n",
       "50382                                                                                                                         @Ambee789 AGREED &amp; AGREED! \n",
       "16000                                                                                        .@TroublePandaPR haha yeah you're the best pimper i've ever had \n",
       "80242                                                                                         @cara_hamilton Hah! No worries  It's kinda hard to get used to.\n",
       "21525                                                                                                  @_ctchmeifyoucn play DS? I'm eating yummy gobstoppers \n",
       "82569                                                                                                                    @cherry_wine The revolution begins. \n",
       "66101                                                                              @BrianNippon never mind my last question. I see where you answered claire \n",
       "77009                 @ByTowne tried a direct message but am having trouble with it   My name is Catherine Di Cesare. does the pass need to be used tomorrow?\n",
       "38687                                                                                                                    @alperdotr i got bored   lunch time?\n",
       "73121                                                                                                                    @AngrySnout I just got called dense \n",
       "64721                                                                                                                             @bradiewebbstack your cool \n",
       "38979                                                          @Aluwir Good question.  It's a short name for my blog Postcards ...  http://tinyurl.com/ctehye\n",
       "23177                                                                        @aanneeB ok  i kinda thought so... i mean those 2? ... it would be funny though.\n",
       "96425                                                                    @Croppley My BlogTV broke when Sam started cohosting so at least I didn't miss much \n",
       "40670                                                   @Amish_roadkill I don't know... I'm pretty fucking amazing at everything I put my hands and mind to. \n",
       "26389                                        @adrianaaXO D: whats yours on, i'd laugh if it's on apple mac -__- mines a major it's not due till this tuesday \n",
       "28918                                                                                                            @ACawlina Hahaha ohhh man i suck! EPIC FAIL \n",
       "70072                 @Andrew05151 i thought you were gonna stick with the apocalypse theme? but i like it  i love you, have a good time at the wedding tmrw!\n",
       "61103    @bigolpoofter I see that my stream of consciousness isn't the only one that needs a dam...Now I have &quot;Can't Touch This&quot; stuck in my head. \n",
       "4647                     #chuckmemondays sounds fun but I didn't get off work early enough to take part. Just leaving work now and gotta commute an hr. Boo! \n",
       "78690                                                                                                                       @CaleighBenson love you too, boo \n",
       "58673                 @Ben_Junior Lol, for a second I thought Inverness would be having Winter again lol, and yeah Youtube's being a pain today, no idea why \n",
       "52998                                                                                           @ayyjaygee Aww that is so sweet of you to do, thanks so much \n",
       "12755                                                                                                                      *sigh* i want to hug her so bad.. \n",
       "87016                                                                                                            @CharityMyLove  Hell YAHH...WE LOVE K YOUNG \n",
       "58218                                                                                                     @Bekemeyer You're too kind, sir. Hope she digs it. \n",
       "6931                                                        #ohac track 2 Tirthankar says you should participate in OHAC for the &quot;bragging rights&quot; \n",
       "56216                                                                                                                                            @baketastic \n",
       "61959                                                                                  @BigJacks I'm good! Any fun events coming up? I've yet to attend one! \n",
       "54891                                               @BananasMel yep ex grilfriend..wonder how I was able to stay so long...I wasted  my time but..whatever.. \n",
       "26930                                                                                                @AEphotoWPG its my bday on Sunday and I have nooo plans \n",
       "41249                                                                                        @amys_bus_ticket  Can't wait to try! (to decipher your tweets!) \n",
       "98824                                                                                                     @courtneyburger gotta be allergies I have them too \n",
       "7107                    #Sims 3: Lots of fun.  Amazingly flexible item/house designer.  Need more men's hair opt., deeper gameplay.  My Sim is dating a cop. \n",
       "91386                                                                          @baloo2k No probs hope that you enjoy tweetdeck - I still have loads to learn \n",
       "63762                                                                                 @bodomgaiden im the only person ur following apart from trivium people \n",
       "8391                                                                                             #YouTube is down for #maintenance and will be back shortly. \n",
       "21830                                                @30SECONDSTOMARS sounds really good!! much better then Kanye West!! I can't wait to hear your new songs \n",
       "93989                                            @chriswhite65 I know  Useful if mugged though, surely ;) Product description: 95% metal, 4% cotton, 1% silk!\n",
       "72354                                                                                                            @CalvinHollywood ach geht doch grad fod. 77 \n",
       "38707                                                               @amp451 your welcome an its not like pink panther an their is singing but its still good \n",
       "Name: SentimentText, dtype: object"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will now take a look at random tweets to gain more insights\n",
    "\n",
    "rand_indexs = np.random.randint(1,len(train),50).tolist()\n",
    "train[\"SentimentText\"][rand_indexs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "83946149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3281, ':/'),\n",
       " (2874, 'x '),\n",
       " (2626, ': '),\n",
       " (1339, 'x@'),\n",
       " (1214, 'xx'),\n",
       " (1162, 'xa'),\n",
       " (984, ';3'),\n",
       " (887, 'xp'),\n",
       " (842, 'xo'),\n",
       " (713, ';)'),\n",
       " (483, 'xe'),\n",
       " (431, ';I'),\n",
       " (353, ';.'),\n",
       " (254, 'xD'),\n",
       " (251, 'x.'),\n",
       " (245, '::'),\n",
       " (234, 'X '),\n",
       " (217, ';t'),\n",
       " (209, ';s'),\n",
       " (185, ':O'),\n",
       " (176, ':3'),\n",
       " (166, ';D'),\n",
       " (159, \":'\"),\n",
       " (157, 'XD'),\n",
       " (146, 'x3'),\n",
       " (142, ':p'),\n",
       " (126, \":'(\"),\n",
       " (118, ':@'),\n",
       " (117, 'xh'),\n",
       " (117, ':S'),\n",
       " (109, 'xm'),\n",
       " (104, ';p'),\n",
       " (104, ';-)'),\n",
       " (92, ':|'),\n",
       " (91, 'x,'),\n",
       " (89, ';P'),\n",
       " (76, 'xd'),\n",
       " (75, ';o'),\n",
       " (75, ';d'),\n",
       " (71, ':o'),\n",
       " (65, 'XX'),\n",
       " (63, ':L'),\n",
       " (59, 'Xx'),\n",
       " (59, ':1'),\n",
       " (58, ':]'),\n",
       " (57, ':s'),\n",
       " (56, ':0'),\n",
       " (54, 'XO'),\n",
       " (44, ';;'),\n",
       " (43, ';('),\n",
       " (38, ':-D'),\n",
       " (37, 'xk'),\n",
       " (36, 'XT'),\n",
       " (35, 'x?'),\n",
       " (35, 'x)'),\n",
       " (34, 'x2'),\n",
       " (33, ';/'),\n",
       " (32, 'x:'),\n",
       " (32, ':\\\\'),\n",
       " (31, 'x-'),\n",
       " (27, 'Xo'),\n",
       " (27, 'XP'),\n",
       " (27, ':-/'),\n",
       " (26, ':-P'),\n",
       " (25, ':*'),\n",
       " (23, 'xX'),\n",
       " (22, \":')\"),\n",
       " (17, 'xP'),\n",
       " (16, ':['),\n",
       " (16, ':-p'),\n",
       " (14, 'x]'),\n",
       " (14, 'XM'),\n",
       " (13, ':-O'),\n",
       " (12, 'x('),\n",
       " (12, 'X1'),\n",
       " (12, ':x'),\n",
       " (11, 'XS'),\n",
       " (11, ':l'),\n",
       " (10, 'x*'),\n",
       " (10, 'X.'),\n",
       " (10, ':b'),\n",
       " (10, ':T'),\n",
       " (9, ';]'),\n",
       " (9, ':I'),\n",
       " (8, ':C'),\n",
       " (7, ';-('),\n",
       " (7, ':-|'),\n",
       " (6, 'X,'),\n",
       " (6, ':-o'),\n",
       " (6, ':-\\\\'),\n",
       " (6, ':-*'),\n",
       " (6, ':$'),\n",
       " (5, 'XL'),\n",
       " (5, ':d'),\n",
       " (5, ':X'),\n",
       " (5, ':H'),\n",
       " (5, ':?'),\n",
       " (5, ':-S'),\n",
       " (4, ';-D'),\n",
       " (3, ':Z'),\n",
       " (3, ':E'),\n",
       " (3, ':-s'),\n",
       " (3, ':-['),\n",
       " (3, ':-X'),\n",
       " (2, 'X5'),\n",
       " (2, 'X-('),\n",
       " (2, \"X's\"),\n",
       " (2, ';-;'),\n",
       " (2, ':}'),\n",
       " (2, ':D'),\n",
       " (2, ':;'),\n",
       " (2, \":'D\"),\n",
       " (1, 'x|'),\n",
       " (1, \"x'd\"),\n",
       " (1, \"x'D\"),\n",
       " (1, ';-|'),\n",
       " (1, ';-/'),\n",
       " (1, ':-x'),\n",
       " (1, ':-h'),\n",
       " (1, ':-]'),\n",
       " (1, ':-W'),\n",
       " (1, ':-$'),\n",
       " (1, ':('),\n",
       " (1, \":'[\")]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are gonna find what emoticons are used in our dataset\n",
    "\n",
    "import re\n",
    "tweets_text = train.SentimentText.str.cat()\n",
    "emos = set(re.findall(r\" ([xX:;][-']?.) \",tweets_text))\n",
    "emos_count = []\n",
    "for emo in emos:\n",
    "    emos_count.append((tweets_text.count(emo), emo))\n",
    "sorted(emos_count,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "29b980c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy emoticons: {';-D', ';p', ':p', ':d', 'x)', ':D', 'xD', ';D', ';P', ';d', ':-D', ';)', 'xd', 'XD', ';-)'}\n",
      "Sad emoticons: {':/', \":'(\", ':(', ':|'}\n"
     ]
    }
   ],
   "source": [
    "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n",
    "SAD_EMO = r\" (:'?[/|\\(]) \"\n",
    "print(\"Happy emoticons:\", set(re.findall(HAPPY_EMO, tweets_text)))\n",
    "print(\"Sad emoticons:\", set(re.findall(SAD_EMO, tweets_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee8302e",
   "metadata": {},
   "source": [
    "# Most Used Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "12a0adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "def most_used_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    frequency_dist = nltk.FreqDist(tokens)\n",
    "    print(\"There is %d different words\" % len(set(tokens)))\n",
    "    return sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "12bd9137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 129018 different words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " '!',\n",
       " '.',\n",
       " 'I',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'you',\n",
       " '?',\n",
       " 'a',\n",
       " 'it',\n",
       " 'i',\n",
       " ';',\n",
       " 'and',\n",
       " '&',\n",
       " '...',\n",
       " 'my',\n",
       " 'for',\n",
       " 'is',\n",
       " 'that',\n",
       " \"'s\",\n",
       " \"n't\",\n",
       " 'in',\n",
       " 'me',\n",
       " 'of',\n",
       " 'have',\n",
       " 'on',\n",
       " 'quot',\n",
       " \"'m\",\n",
       " 'so',\n",
       " ':',\n",
       " 'but',\n",
       " '#',\n",
       " 'do',\n",
       " 'was',\n",
       " 'be',\n",
       " '..',\n",
       " 'not',\n",
       " 'your',\n",
       " 'are',\n",
       " 'just',\n",
       " 'with',\n",
       " 'like',\n",
       " '-',\n",
       " 'at',\n",
       " '*',\n",
       " 'too',\n",
       " 'get',\n",
       " 'good',\n",
       " 'u',\n",
       " 'up',\n",
       " 'know',\n",
       " 'all',\n",
       " 'this',\n",
       " 'now',\n",
       " 'no',\n",
       " 'we',\n",
       " 'out',\n",
       " ')',\n",
       " 'love',\n",
       " 'lol',\n",
       " 'can',\n",
       " 'what',\n",
       " 'one',\n",
       " '(',\n",
       " 'will',\n",
       " 'go',\n",
       " 'about',\n",
       " 'did',\n",
       " 'got',\n",
       " \"'ll\",\n",
       " 'there',\n",
       " 'amp',\n",
       " 'day',\n",
       " 'http',\n",
       " 'see',\n",
       " \"'re\",\n",
       " 'if',\n",
       " 'time',\n",
       " 'they',\n",
       " 'think',\n",
       " 'as',\n",
       " 'when',\n",
       " 'from',\n",
       " 'You',\n",
       " 'It',\n",
       " 'going',\n",
       " 'really',\n",
       " 'well',\n",
       " 'am',\n",
       " 'work',\n",
       " 'had',\n",
       " 'would',\n",
       " 'how',\n",
       " 'he',\n",
       " 'here',\n",
       " 'thanks',\n",
       " 'some',\n",
       " '....',\n",
       " 'haha']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_used_words(train.SentimentText.str.cat())[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b3ac42",
   "metadata": {},
   "source": [
    "# Stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b3d0eccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 129018 different words\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "mw = most_used_words(train.SentimentText.str.cat())\n",
    "most_words = []\n",
    "for w in mw:\n",
    "    if len(most_words) == 1000:\n",
    "        break\n",
    "    if w in stopwords.words(\"english\"):\n",
    "        continue\n",
    "    else:\n",
    "        most_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "caa570d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " \"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '....',\n",
       " '.....',\n",
       " '......',\n",
       " '/',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '12',\n",
       " '1st',\n",
       " '2',\n",
       " '20',\n",
       " '2nd',\n",
       " '3',\n",
       " '30',\n",
       " '30SECONDSTOMARS',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'AND',\n",
       " 'Ah',\n",
       " 'AlexAllTimeLow',\n",
       " 'All',\n",
       " 'Also',\n",
       " 'Alyssa_Milano',\n",
       " 'Am',\n",
       " 'And',\n",
       " 'Are',\n",
       " 'As',\n",
       " 'At',\n",
       " 'Aw',\n",
       " 'Awesome',\n",
       " 'Aww',\n",
       " 'Awww',\n",
       " 'BSB',\n",
       " 'Birthday',\n",
       " 'But',\n",
       " 'Ca',\n",
       " 'Can',\n",
       " 'Chris',\n",
       " 'Come',\n",
       " 'Congrats',\n",
       " 'Cool',\n",
       " 'D',\n",
       " 'DM',\n",
       " 'DO',\n",
       " 'Damn',\n",
       " 'Day',\n",
       " 'Did',\n",
       " 'Do',\n",
       " 'Enjoy',\n",
       " 'FF',\n",
       " 'Follow',\n",
       " 'FollowFriday',\n",
       " 'For',\n",
       " 'Friday',\n",
       " 'Get',\n",
       " 'Glad',\n",
       " 'Go',\n",
       " 'God',\n",
       " 'Good',\n",
       " 'Got',\n",
       " 'Great',\n",
       " 'Had',\n",
       " 'Haha',\n",
       " 'Happy',\n",
       " 'Have',\n",
       " 'He',\n",
       " 'Hello',\n",
       " 'Hey',\n",
       " 'Hi',\n",
       " 'Hope',\n",
       " 'How',\n",
       " 'I',\n",
       " 'IS',\n",
       " 'IT',\n",
       " 'If',\n",
       " 'Im',\n",
       " 'In',\n",
       " 'Is',\n",
       " 'It',\n",
       " 'Its',\n",
       " 'July',\n",
       " 'June',\n",
       " 'Just',\n",
       " 'Keep',\n",
       " 'LA',\n",
       " 'LMAO',\n",
       " 'LOL',\n",
       " 'LOVE',\n",
       " 'Let',\n",
       " 'Like',\n",
       " 'Lol',\n",
       " 'London',\n",
       " 'Love',\n",
       " 'ME',\n",
       " 'MY',\n",
       " 'Maybe',\n",
       " 'Me',\n",
       " 'Monday',\n",
       " 'Morning',\n",
       " 'My',\n",
       " 'NO',\n",
       " 'NOT',\n",
       " 'New',\n",
       " 'Nice',\n",
       " 'Night',\n",
       " 'No',\n",
       " 'Not',\n",
       " 'Now',\n",
       " 'O',\n",
       " 'OK',\n",
       " 'OMG',\n",
       " 'Of',\n",
       " 'Oh',\n",
       " 'Ok',\n",
       " 'On',\n",
       " 'Once',\n",
       " 'One',\n",
       " 'Only',\n",
       " 'Or',\n",
       " 'Please',\n",
       " 'Poor',\n",
       " 'Really',\n",
       " 'S',\n",
       " 'SO',\n",
       " 'Saturday',\n",
       " 'See',\n",
       " 'She',\n",
       " 'So',\n",
       " 'Sorry',\n",
       " 'Sounds',\n",
       " 'Still',\n",
       " 'Sunday',\n",
       " 'THAT',\n",
       " 'THE',\n",
       " 'TO',\n",
       " 'TV',\n",
       " 'Tell',\n",
       " 'Thank',\n",
       " 'Thanks',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Then',\n",
       " 'There',\n",
       " 'They',\n",
       " 'This',\n",
       " 'To',\n",
       " 'Too',\n",
       " 'Twitter',\n",
       " 'U',\n",
       " 'UK',\n",
       " 'US',\n",
       " 'Very',\n",
       " 'Was',\n",
       " 'We',\n",
       " 'Welcome',\n",
       " 'Well',\n",
       " 'What',\n",
       " 'When',\n",
       " 'Where',\n",
       " 'Who',\n",
       " 'Why',\n",
       " 'Will',\n",
       " 'Wish',\n",
       " 'Would',\n",
       " 'Wow',\n",
       " 'XD',\n",
       " 'YAY',\n",
       " 'YES',\n",
       " 'YOU',\n",
       " 'Yay',\n",
       " 'Yeah',\n",
       " 'Yep',\n",
       " 'Yes',\n",
       " 'You',\n",
       " 'Your',\n",
       " '[',\n",
       " ']',\n",
       " 'able',\n",
       " 'absolutely',\n",
       " 'account',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'afternoon',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ah',\n",
       " 'ahh',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'album',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'amp',\n",
       " 'andyclemmensen',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'aplusk',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appreciate',\n",
       " 'around',\n",
       " 'ashleytisdale',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'aw',\n",
       " 'awake',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'awwww',\n",
       " 'b',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'babygirlparis',\n",
       " 'back',\n",
       " 'backstreetboys',\n",
       " 'bad',\n",
       " 'band',\n",
       " 'bday',\n",
       " 'beach',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'bed',\n",
       " 'beer',\n",
       " 'behind',\n",
       " 'believe',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'big',\n",
       " 'billyraycyrus',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'black',\n",
       " 'blog',\n",
       " 'blue',\n",
       " 'body',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'books',\n",
       " 'bored',\n",
       " 'boring',\n",
       " 'bought',\n",
       " 'bout',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boys',\n",
       " 'bradiewebbstack',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'bring',\n",
       " 'bro',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brother',\n",
       " 'btw',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'buy',\n",
       " 'c',\n",
       " 'ca',\n",
       " 'cake',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'cant',\n",
       " 'car',\n",
       " 'care',\n",
       " 'case',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'cause',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'chat',\n",
       " 'check',\n",
       " 'chocolate',\n",
       " 'city',\n",
       " 'class',\n",
       " 'close',\n",
       " 'club',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'computer',\n",
       " 'concert',\n",
       " 'congrats',\n",
       " 'cool',\n",
       " 'cos',\n",
       " 'could',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'coz',\n",
       " 'crap',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuz',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'damn',\n",
       " 'dance',\n",
       " 'date',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dear',\n",
       " 'def',\n",
       " 'definitely',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'different',\n",
       " 'dinner',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'dream',\n",
       " 'dreams',\n",
       " 'drink',\n",
       " 'drive',\n",
       " 'dude',\n",
       " 'due',\n",
       " 'dunno',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eating',\n",
       " 'eh',\n",
       " 'either',\n",
       " 'else',\n",
       " 'em',\n",
       " 'email',\n",
       " 'end',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enough',\n",
       " 'especially',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'exactly',\n",
       " 'exam',\n",
       " 'exams',\n",
       " 'except',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'eye',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'fact',\n",
       " 'fail',\n",
       " 'fair',\n",
       " 'fall',\n",
       " 'family',\n",
       " 'fan',\n",
       " 'fans',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'favorite',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'feels',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'figure',\n",
       " 'film',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'first',\n",
       " 'fix',\n",
       " 'flight',\n",
       " 'follow',\n",
       " 'followers',\n",
       " 'followfriday',\n",
       " 'following',\n",
       " 'food',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgot',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'free',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'front',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'funny',\n",
       " 'future',\n",
       " 'game',\n",
       " 'gave',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'girl',\n",
       " 'girls',\n",
       " 'give',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'go',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gon',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'goodnight',\n",
       " 'gorgeous',\n",
       " 'got',\n",
       " 'great',\n",
       " 'green',\n",
       " 'gt',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'ha',\n",
       " 'haha',\n",
       " 'hahah',\n",
       " 'hahaha',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hands',\n",
       " 'hang',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hate',\n",
       " 'havent',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'hehe',\n",
       " 'hell',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'hit',\n",
       " 'hmm',\n",
       " 'hold',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hopefully',\n",
       " 'hoping',\n",
       " 'horrible',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'http',\n",
       " 'hug',\n",
       " 'huge',\n",
       " 'hugs',\n",
       " 'huh',\n",
       " 'hun',\n",
       " 'hungry',\n",
       " 'hurt',\n",
       " 'hurts',\n",
       " 'iPhone',\n",
       " 'ice',\n",
       " 'idea',\n",
       " 'idk',\n",
       " 'ill',\n",
       " 'im',\n",
       " 'inaperfectworld',\n",
       " 'indeed',\n",
       " 'info',\n",
       " 'instead',\n",
       " 'interesting',\n",
       " 'internet',\n",
       " 'invite',\n",
       " 'iremember',\n",
       " 'ive',\n",
       " 'jealous',\n",
       " 'job',\n",
       " 'join',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kid',\n",
       " 'kidding',\n",
       " 'kids',\n",
       " 'kill',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knows',\n",
       " 'lady',\n",
       " 'lame',\n",
       " 'laptop',\n",
       " 'last',\n",
       " 'late',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'laugh',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'leaving',\n",
       " 'left',\n",
       " 'less',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'life',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'lil',\n",
       " 'line',\n",
       " 'link',\n",
       " 'list',\n",
       " 'listen',\n",
       " 'listening',\n",
       " 'little',\n",
       " 'live',\n",
       " 'living',\n",
       " 'lmao',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'lose',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lots',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'lovely',\n",
       " 'loves',\n",
       " 'lt',\n",
       " 'luck',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'luv',\n",
       " 'mad',\n",
       " 'made',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'making',\n",
       " 'man',\n",
       " 'many',\n",
       " 'mate',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meant',\n",
       " 'meet',\n",
       " 'meeting',\n",
       " 'mention',\n",
       " 'message',\n",
       " 'met',\n",
       " 'might',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'miss',\n",
       " 'missed',\n",
       " 'missing',\n",
       " 'mom',\n",
       " 'moment',\n",
       " 'monday',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'mood',\n",
       " 'morning',\n",
       " 'move',\n",
       " 'movie',\n",
       " 'movies',\n",
       " 'moving',\n",
       " 'much',\n",
       " 'mum',\n",
       " 'music',\n",
       " 'musicmonday',\n",
       " 'must',\n",
       " 'myspace',\n",
       " 'myweakness',\n",
       " 'n',\n",
       " \"n't\",\n",
       " 'na',\n",
       " 'name',\n",
       " 'near',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'nite',\n",
       " 'nope',\n",
       " 'nothing',\n",
       " 'number',\n",
       " 'office',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'omg',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'online',\n",
       " 'open',\n",
       " 'order',\n",
       " 'others',\n",
       " 'outside',\n",
       " 'p',\n",
       " 'page',\n",
       " 'pain',\n",
       " 'parents',\n",
       " 'part',\n",
       " 'party',\n",
       " 'pass',\n",
       " 'past',\n",
       " 'pay',\n",
       " 'people',\n",
       " 'perfect',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'photo',\n",
       " 'photos',\n",
       " 'pic',\n",
       " 'pick',\n",
       " 'pics',\n",
       " 'picture',\n",
       " 'pictures',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'plans',\n",
       " 'play',\n",
       " 'played',\n",
       " 'playing',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'pool',\n",
       " 'poor',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'ppl',\n",
       " 'pretty',\n",
       " 'prob',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'profile',\n",
       " 'proud',\n",
       " 'put',\n",
       " 'question',\n",
       " 'quite',\n",
       " 'quot',\n",
       " 'r',\n",
       " 'radio',\n",
       " 'rain',\n",
       " 'raining',\n",
       " 'random',\n",
       " 'rather',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'red',\n",
       " 'remember',\n",
       " 'reply',\n",
       " 'rest',\n",
       " 'ride',\n",
       " 'right',\n",
       " 'rock',\n",
       " 'room',\n",
       " 'run',\n",
       " 'running',\n",
       " 'sad',\n",
       " 'sadly',\n",
       " 'safe',\n",
       " 'said',\n",
       " 'save',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'scared',\n",
       " 'school',\n",
       " 'season',\n",
       " 'second',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'send',\n",
       " 'sense',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'set',\n",
       " 'sexy',\n",
       " 'shall',\n",
       " 'shame',\n",
       " 'share',\n",
       " 'sharing',\n",
       " 'shit',\n",
       " 'shopping',\n",
       " 'short',\n",
       " 'show',\n",
       " 'shows',\n",
       " 'shut',\n",
       " 'sick',\n",
       " 'side',\n",
       " 'sigh',\n",
       " 'sign',\n",
       " 'silly',\n",
       " 'since',\n",
       " 'single',\n",
       " 'sis',\n",
       " 'sister',\n",
       " 'site',\n",
       " 'sitting',\n",
       " 'sleep',\n",
       " 'sleeping',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smile',\n",
       " 'sold',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'son',\n",
       " 'song',\n",
       " 'songs',\n",
       " 'soo',\n",
       " 'soon',\n",
       " 'sooo',\n",
       " 'soooo',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'sounds',\n",
       " 'special',\n",
       " 'squarespace',\n",
       " 'start',\n",
       " 'started',\n",
       " 'starting',\n",
       " 'stay',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'store',\n",
       " 'story',\n",
       " 'stuck',\n",
       " 'study',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'suck',\n",
       " 'sucks',\n",
       " 'summer',\n",
       " 'sun',\n",
       " 'sunday',\n",
       " 'sunny',\n",
       " 'super',\n",
       " 'support',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'sweet',\n",
       " 'sweetie',\n",
       " 'ta',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tea',\n",
       " 'team',\n",
       " 'tell',\n",
       " 'text',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thats',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'tho',\n",
       " 'though',\n",
       " 'thought',\n",
       " 'three',\n",
       " 'thx',\n",
       " 'tickets',\n",
       " 'til',\n",
       " 'till',\n",
       " 'time',\n",
       " 'times',\n",
       " 'tired',\n",
       " 'today',\n",
       " 'together',\n",
       " 'told',\n",
       " 'tomorrow',\n",
       " 'tonight',\n",
       " 'took',\n",
       " 'top',\n",
       " 'totally',\n",
       " 'touch',\n",
       " 'tour',\n",
       " 'town',\n",
       " 'train',\n",
       " 'tried',\n",
       " 'trip',\n",
       " 'true',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'turn',\n",
       " 'tv',\n",
       " 'tweet',\n",
       " 'tweeting',\n",
       " 'tweets',\n",
       " 'twitter',\n",
       " 'two',\n",
       " 'type',\n",
       " 'u',\n",
       " 'ugh',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'update',\n",
       " 'updates',\n",
       " 'upset',\n",
       " 'ur',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'version',\n",
       " 'via',\n",
       " 'video',\n",
       " 'vip',\n",
       " 'visit',\n",
       " 'voice',\n",
       " 'vote',\n",
       " 'w/',\n",
       " 'wait',\n",
       " 'waiting',\n",
       " 'wake',\n",
       " 'walk',\n",
       " 'wan',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wants',\n",
       " 'warm',\n",
       " 'watch',\n",
       " 'watched',\n",
       " 'watching',\n",
       " 'water',\n",
       " 'way',\n",
       " 'wear',\n",
       " 'weather',\n",
       " 'website',\n",
       " 'wedding',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weeks',\n",
       " 'weird',\n",
       " 'welcome',\n",
       " 'well',\n",
       " 'went',\n",
       " 'whatever',\n",
       " 'whats',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'win',\n",
       " 'wine',\n",
       " 'wish',\n",
       " 'wit',\n",
       " 'without',\n",
       " 'wo',\n",
       " 'woke',\n",
       " 'woman',\n",
       " 'wonder',\n",
       " 'wonderful',\n",
       " 'wont',\n",
       " 'word',\n",
       " 'words',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'works',\n",
       " 'world',\n",
       " 'worry',\n",
       " 'worse',\n",
       " 'worst',\n",
       " 'worth',\n",
       " 'would',\n",
       " 'wow',\n",
       " 'write',\n",
       " 'writing',\n",
       " 'wrong',\n",
       " 'x',\n",
       " 'xD',\n",
       " 'xoxo',\n",
       " 'xx',\n",
       " 'xxx',\n",
       " 'ya',\n",
       " 'yay',\n",
       " 'yea',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'years',\n",
       " 'yep',\n",
       " 'yes',\n",
       " 'yesterday',\n",
       " 'yet',\n",
       " 'yo',\n",
       " 'youtube',\n",
       " 'yup',\n",
       " '|',\n",
       " '~',\n",
       " 'Â»']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What we did is to filter only non stop words.\n",
    "# We will now get a look to the top 1000 words\n",
    "\n",
    "sorted(most_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbad52",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "33904f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm defining this function to use it in the \n",
    "# Data Preparation Phase\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def stem_tokenize(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    return [stemmer.lemmatize(token) for token in word_tokenize(text)]\n",
    "\n",
    "def lemmatize_tokenize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c0627",
   "metadata": {},
   "source": [
    "# Preparing the data and Building the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9bea1359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e3559901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "09843099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to do some preprocessing of the tweets.\n",
    "# We will delete useless strings (like @, # ...)\n",
    "# because we think that they will not help\n",
    "# in determining if the person is Happy/Sad\n",
    "\n",
    "class TextPreProc(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, use_mention=False):\n",
    "        self.use_mention = use_mention\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # We can choose between keeping the mentions\n",
    "        # or deleting them\n",
    "        if self.use_mention:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" @tags \")\n",
    "        else:\n",
    "            X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \"\")\n",
    "            \n",
    "        # Keeping only the word after the #\n",
    "        X = X.str.replace(\"#\", \"\")\n",
    "        X = X.str.replace(r\"[-\\.\\n]\", \"\")\n",
    "        # Removing HTML garbage\n",
    "        X = X.str.replace(r\"&\\w+;\", \"\")\n",
    "        # Removing links\n",
    "        X = X.str.replace(r\"https?://\\S*\", \"\")\n",
    "        # replace repeated letters with only two occurences\n",
    "        # heeeelllloooo => heelloo\n",
    "        X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n",
    "        # mark emoticons as happy or sad\n",
    "        X = X.str.replace(HAPPY_EMO, \" happyemoticons \")\n",
    "        X = X.str.replace(SAD_EMO, \" sademoticons \")\n",
    "        X = X.str.lower()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "83d1a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the pipeline that will transform our tweets to something eatable.\n",
    "# You can see that we are using our previously defined stemmer, it will\n",
    "# take care of the stemming process.\n",
    "# For stop words, we let the inverse document frequency do the job\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentiments = train['Sentiment']\n",
    "tweets = train['SentimentText']\n",
    "\n",
    "# I get those parameters from the 'Fine tune the model' part\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmatize_tokenize, ngram_range=(1,2))\n",
    "pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc(use_mention=True)),\n",
    "    ('vectorizer', vectorizer),\n",
    "])\n",
    "\n",
    "# Let's split our data into learning set and testing set\n",
    "# This process is done to test the efficency of our model at the end.\n",
    "# You shouldn't look at the test data only after choosing the final model\n",
    "learn_data, test_data, sentiments_learning, sentiments_test = train_test_split(tweets, sentiments, test_size=0.3)\n",
    "\n",
    "# This will tranform our learning data from simple text to vector\n",
    "# by going through the preprocessing tranformer.\n",
    "learning_data = pipeline.fit_transform(learn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56c139",
   "metadata": {},
   "source": [
    "# Select a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e272cf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== logitic regression ===\n",
      "scores =  [0.81153101 0.81145066 0.8156682  0.81754471 0.80568606 0.81344092\n",
      " 0.81390543 0.81293748 0.8033241  0.81273545]\n",
      "mean =  0.811822402905744\n",
      "variance =  1.6666957859036546e-05\n",
      "score on the learning data (accuracy) =  0.8716567607726597\n",
      "\n",
      "=== bernoulliNB ===\n",
      "scores =  [0.78455616 0.79069221 0.78814155 0.79124737 0.79062134 0.78985251\n",
      " 0.78147578 0.79114961 0.78709979 0.78803459]\n",
      "mean =  0.7882870905034208\n",
      "variance =  9.24495002469739e-06\n",
      "score on the learning data (accuracy) =  0.9021030974968568\n",
      "\n",
      "=== multinomialNB ===\n",
      "scores =  [0.80468489 0.80586655 0.81460862 0.81336612 0.80725977 0.81054513\n",
      " 0.80860167 0.80981595 0.80400445 0.81291484]\n",
      "mean =  0.809166797181055\n",
      "variance =  1.252018422636353e-05\n",
      "score on the learning data (accuracy) =  0.8990598925591496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "lr = LogisticRegression()\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "models = {\n",
    "    'logitic regression': lr,\n",
    "    'bernoulliNB': bnb,\n",
    "    'multinomialNB': mnb,\n",
    "}\n",
    "\n",
    "for model in models.keys():\n",
    "    scores = cross_val_score(models[model], learning_data, sentiments_learning, scoring=\"f1\", cv=10)\n",
    "    print(\"===\", model, \"===\")\n",
    "    print(\"scores = \", scores)\n",
    "    print(\"mean = \", scores.mean())\n",
    "    print(\"variance = \", scores.var())\n",
    "    models[model].fit(learning_data, sentiments_learning)\n",
    "    print(\"score on the learning data (accuracy) = \", accuracy_score(models[model].predict(learning_data), sentiments_learning))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c79cf",
   "metadata": {},
   "source": [
    "# Fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6a7278fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# None of those models is likely to be overfitting, I will choose the multinomialNB.\n",
    "# I'm going to use the GridSearchCV to choose the best parameters to use.\n",
    "# What the GridSearchCV does is trying different set of parameters, and for\n",
    "# each one, it runs a cross validation and estimate the score.\n",
    "# At the end we can see what are the best parameter and use them to build a better classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "69ff3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_pre_processing__use_mention': True, 'vectorizer__max_features': None, 'vectorizer__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search_pipeline = Pipeline([\n",
    "    ('text_pre_processing', TextPreProc()),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB()),\n",
    "])\n",
    "\n",
    "params = [\n",
    "    {\n",
    "        'text_pre_processing__use_mention': [True, False],\n",
    "        'vectorizer__max_features': [1000, 2000, 5000, 10000, 20000, None],\n",
    "        'vectorizer__ngram_range': [(1,1), (1,2)],\n",
    "    },\n",
    "]\n",
    "grid_search = GridSearchCV(grid_search_pipeline, params, cv=5, scoring='f1')\n",
    "grid_search.fit(learn_data, sentiments_learning)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6580fdb8",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "631ed3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(learning_data, sentiments_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ad04bccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7551088442177551"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data = pipeline.transform(test_data)\n",
    "mnb.score(testing_data, sentiments_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "99b03be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ItemID  Sentiment\n",
      "0            1          0\n",
      "1            2          0\n",
      "2            3          0\n",
      "3            4          0\n",
      "4            5          0\n",
      "...        ...        ...\n",
      "299984  299996          1\n",
      "299985  299997          1\n",
      "299986  299998          1\n",
      "299987  299999          1\n",
      "299988  300000          1\n",
      "\n",
      "[299989 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Predecting on the test.csv\n",
    "\n",
    "sub_data = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "sub_learning = pipeline.transform(sub_data.SentimentText)\n",
    "sub = pd.DataFrame(sub_data.ItemID, columns=(\"ItemID\", \"Sentiment\"))\n",
    "sub[\"Sentiment\"] = mnb.predict(sub_learning)\n",
    "print(sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
